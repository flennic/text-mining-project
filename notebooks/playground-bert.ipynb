{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readme.txt  test.csv  train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls ../src/data/original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p-bert_p-200'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = np.array([0.85, 0.1, 0.05])\n",
    "indicator = \"bert\"\n",
    "padding = \"200\"\n",
    "file_suffix = \"p-{}_p-{}\".format(indicator, padding)\n",
    "file_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_train_path =  u\"../src/data/original/train.csv\"\n",
    "orig_test_path = u\"../src/data/original/test.csv\"\n",
    "prep_train_path =  u\"../src/data/processed/train_{}_s{}.csv\".format(file_suffix, str(splits[0]))\n",
    "prep_val_path =  u\"../src/data/processed/val_{}_s{}.csv\".format(file_suffix, str(splits[1]))\n",
    "prep_test_path =  u\"../src/data/processed/test_{}_s{}.csv\".format(file_suffix, str(splits[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.6 s, sys: 1.14 s, total: 23.8 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv(orig_train_path, header=None)\n",
    "test = pd.read_csv(orig_test_path, header=None)\n",
    "\n",
    "data = pd.concat([train, test])\n",
    "\n",
    "del train, test\n",
    "\n",
    "data.columns = [\"label\", \"title\", \"review\"]\n",
    "data.drop(columns=['title'], inplace=True)\n",
    "data[\"label\"] = data[\"label\"] - 1\n",
    "data = data.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "train, val, test = np.array_split(data, (splits[:-1].cumsum() * len(data)).astype(int))\n",
    "\n",
    "del data\n",
    "del train, val\n",
    "\n",
    "#train = train.values.tolist()\n",
    "#val = val.values.tolist()\n",
    "test = test.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3,\n",
       " 'A very unexpected continuation of the first book, but with the same emotional feel and realism of the times and hearts of the characters. It has many new unexpected turns, with lots of suspense, but leaves you completely satisfied with how everything ties together in the end. This was better than the first book in the area of sensuality, as it was not completely focused on it, this time. In my view it was a more balanced look at life. I was a little disappointed with some characters left hanging at the end, but overall it was a well written story as part of a series.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test[0]\n",
    "tokenizerBert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "embedding_model = BertModel.from_pretrained('bert-base-uncased').cuda()\n",
    "unk_token = tokenizerBert._unk_token\n",
    "pad_token = tokenizerBert._pad_token\n",
    "padding = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very unexpected continuation of the first book, but with the same emotional feel and realism of the times and hearts of the characters. It has many new unexpected turns, with lots of suspense, but leaves you completely satisfied with how everything ties together in the end. This was better than the first book in the area of sensuality, as it was not completely focused on it, this time. In my view it was a more balanced look at life. I was a little disappointed with some characters left hanging at the end, but overall it was a well written story as part of a series.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 575 ms, sys: 0 ns, total: 575 ms\n",
      "Wall time: 575 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize\n",
    "batch = []\n",
    "\n",
    "for i in range(256):\n",
    "    batch.append(tokenizerBert.encode(sample[1], add_special_tokens=False, max_length=padding, pad_to_max_length=True))\n",
    "\n",
    "#sentence = tokenizer.encode(sample[1], add_special_tokens=False, max_length=padding, pad_to_max_length=True)\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gpu = torch.LongTensor(batch).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 200])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gpu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test[0]\n",
    "tokenizerBert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "embedding_model = DistilBertModel.from_pretrained('distilbert-base-uncased').cuda()\n",
    "unk_token = tokenizerBert._unk_token\n",
    "pad_token = tokenizerBert._pad_token\n",
    "padding = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very unexpected continuation of the first book, but with the same emotional feel and realism of the times and hearts of the characters. It has many new unexpected turns, with lots of suspense, but leaves you completely satisfied with how everything ties together in the end. This was better than the first book in the area of sensuality, as it was not completely focused on it, this time. In my view it was a more balanced look at life. I was a little disappointed with some characters left hanging at the end, but overall it was a well written story as part of a series.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', ',', 'my', 'dog', 'is', 'cute', '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerBert.tokenize(\"Hello, my dog is cute.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "__padding = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizerBert.encode(\"Hello, my dog is cute.\", add_special_tokens=True,  max_length=__padding, pad_to_max_length=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,  1012,   102]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tokenizerBert.encode(\"Hello, my dog is cute.\", add_special_tokens=True)).unsqueeze(0).to('cuda')  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerBert.convert_tokens_to_ids(\"[CLS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1037,\n",
       " 2200,\n",
       " 9223,\n",
       " 13633,\n",
       " 1997,\n",
       " 1996,\n",
       " 2034,\n",
       " 2338,\n",
       " 1010,\n",
       " 2021,\n",
       " 2007,\n",
       " 1996,\n",
       " 2168,\n",
       " 6832,\n",
       " 2514,\n",
       " 1998,\n",
       " 15650,\n",
       " 1997,\n",
       " 1996,\n",
       " 2335,\n",
       " 1998,\n",
       " 8072,\n",
       " 1997,\n",
       " 1996,\n",
       " 3494,\n",
       " 1012,\n",
       " 2009,\n",
       " 2038,\n",
       " 2116,\n",
       " 2047,\n",
       " 9223,\n",
       " 4332,\n",
       " 1010,\n",
       " 2007,\n",
       " 7167,\n",
       " 1997,\n",
       " 23873,\n",
       " 1010,\n",
       " 2021,\n",
       " 3727,\n",
       " 2017,\n",
       " 3294,\n",
       " 8510,\n",
       " 2007,\n",
       " 2129,\n",
       " 2673,\n",
       " 7208,\n",
       " 2362,\n",
       " 1999,\n",
       " 1996,\n",
       " 2203,\n",
       " 1012,\n",
       " 2023,\n",
       " 2001,\n",
       " 2488,\n",
       " 2084,\n",
       " 1996,\n",
       " 2034,\n",
       " 2338,\n",
       " 1999,\n",
       " 1996,\n",
       " 2181,\n",
       " 1997,\n",
       " 18753,\n",
       " 3012,\n",
       " 1010,\n",
       " 2004,\n",
       " 2009,\n",
       " 2001,\n",
       " 2025,\n",
       " 3294,\n",
       " 4208,\n",
       " 2006,\n",
       " 2009,\n",
       " 1010,\n",
       " 2023,\n",
       " 2051,\n",
       " 1012,\n",
       " 1999,\n",
       " 2026,\n",
       " 3193,\n",
       " 2009,\n",
       " 2001,\n",
       " 1037,\n",
       " 2062,\n",
       " 12042,\n",
       " 2298,\n",
       " 2012,\n",
       " 2166,\n",
       " 1012,\n",
       " 1045,\n",
       " 2001,\n",
       " 1037,\n",
       " 2210,\n",
       " 9364,\n",
       " 2007,\n",
       " 2070,\n",
       " 3494,\n",
       " 2187,\n",
       " 5689,\n",
       " 2012,\n",
       " 1996,\n",
       " 2203,\n",
       " 1010,\n",
       " 2021,\n",
       " 3452,\n",
       " 2009,\n",
       " 2001,\n",
       " 1037,\n",
       " 2092,\n",
       " 2517,\n",
       " 2466,\n",
       " 2004,\n",
       " 2112,\n",
       " 1997,\n",
       " 1037,\n",
       " 2186,\n",
       " 1012,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizerBert.encode(sample[1], add_special_tokens=False, max_length=padding, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 575 ms, sys: 4.16 ms, total: 579 ms\n",
      "Wall time: 577 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize\n",
    "batch = []\n",
    "\n",
    "for i in range(256):\n",
    "    batch.append(tokenizerBert.encode(sample[1], add_special_tokens=False, max_length=padding, pad_to_max_length=True))\n",
    "\n",
    "#sentence = tokenizer.encode(sample[1], add_special_tokens=False, max_length=padding, pad_to_max_length=True)\n",
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_gpu = torch.LongTensor(batch).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 ms, sys: 364 µs, total: 15.8 ms\n",
      "Wall time: 14.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    res = embedding_model(batch_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.92 ms, sys: 27.3 ms, total: 33.3 ms\n",
      "Wall time: 31.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    res = embedding_model(batch_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = res[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 200, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(embeddings, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del res\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "batchAsList = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(batchAsList[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_out.dat\", \"ab\") as out_file:\n",
    "    for entry in batchAsList:\n",
    "        X = np.array(entry)\n",
    "        np.savetxt(out_file, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np        \n",
    "f=open('asd.dat','ab')\n",
    "for iind in range(4):\n",
    "    a=np.random.rand(10,10)\n",
    "    np.savetxt(f,a)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(row, padding=200,\n",
    "               tokenizer=tokenizerBert,\n",
    "               embedder=embedding_model,\n",
    "               unk_token=tokenizerBert._unk_token,\n",
    "               pad_token=tokenizerBert._pad_token):\n",
    "\n",
    "    # row = [label, review]\n",
    "    \n",
    "    # Tokenize, Vectorize and Pad\n",
    "    sentence_as_int = tokenizer.encode(row[1], add_special_tokens=False, max_length=padding, pad_to_max_length=True)\n",
    "\n",
    "    # X, Y\n",
    "    return sentence_as_int, row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cores = max(1, round(mp.cpu_count() / 2))\n",
    "print(cores)\n",
    "pool = mp.Pool(cores)\n",
    "\n",
    "# Parallelizing, will work as long as the processing is not too fast and fillst the memory :o\n",
    "processed_test = pool.imap(preprocess, test)\n",
    "\n",
    "with open(prep_test_path, \"w\") as out_file:\n",
    "    for X, Y in processed_test:\n",
    "        stringified = [str(entry) for entry in [Y] + X]\n",
    "        out_file.write(\",\".join(stringified) + \"\\n\")\n",
    "    \n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "del test, processed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embedding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
