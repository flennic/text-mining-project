{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = u\"../src/data/processed/train.730000.csv\"\n",
    "path_val = u\"../src/data/processed/val.csv\"\n",
    "path_test = u\"../src/data/processed/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in word vectors. Reduced to 500k due to RAM limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Todo:** Filter for those vocabularies that are actually in the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 1.28 s, total: 16.6 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "import numpy as np\n",
    "embedding_model = gensim.models.KeyedVectors.load_word2vec_format('../src/data/embeddings/GoogleNews-vectors-negative300.bin', binary=True, limit=500_000)\n",
    "embedding_model.add('<oov>', np.mean(embedding_model.vectors, axis=0),replace=False)\n",
    "embedding_model.add('<padding>', np.zeros(300),replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Dataset for the Amazon reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, path, padding=200, padding_token=\"<padding>\", oov_token=\"<oov>\"):\n",
    "        \n",
    "        tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        self.samples = []\n",
    "        \n",
    "        # Load all the data\n",
    "        data = pd.read_csv(path)\n",
    "                \n",
    "        # Tokenize, pad and vectorize each review\n",
    "        for index, row in data.iterrows():\n",
    "            \n",
    "            # Tokenize\n",
    "            sentence = [token.text for token in tokenizer(row[\"review\"])]\n",
    "            \n",
    "            # Pad\n",
    "            sentence = sentence[:padding] + [padding_token]*(padding - len(sentence))\n",
    "            \n",
    "            # Vectorize\n",
    "            row = [row[\"id\"], row[\"label\"], row[\"alpha\"]] + sentence\n",
    "            tensor = self.__row2tensor__(row, oov_token)\n",
    "            \n",
    "            self.samples.append(tensor)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return self.samples[idx]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __row2tensor__(sentence, oov_token):\n",
    "        filled_sentence = [word if embedding_model.vocab.get(word) is not None else oov_token for word in sentence[3:]]\n",
    "        sentence_as_int = [embedding_model.vocab.get(word).index for word in filled_sentence]\n",
    "        return sentence_as_int, sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and saving Training Dataset\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from os import path\n",
    "import pickle\n",
    "\n",
    "pickle_path_train = \"data.train-0_20_500k.pickle\"\n",
    "\n",
    "if path.exists(pickle_path_train):\n",
    "    print(\"Loading Training Dataset\")\n",
    "    with open(pickle_path_train, \"rb\") as pickled:\n",
    "        train_data = pickle.load(pickled)\n",
    "else:\n",
    "    print(\"Creating and saving Training Dataset\")\n",
    "    train_data = AmazonReviewDataset(path_train)\n",
    "    with open(pickle_path_train, \"wb\") as pickled:\n",
    "         pickle.dump(train_data, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from os import path\n",
    "import pickle\n",
    "\n",
    "pickle_path_val = \"data.val-0_10_500k.pickle\"\n",
    "\n",
    "if path.exists(pickle_path_val):\n",
    "    print(\"Loading Validation Dataset\")\n",
    "    with open(pickle_path_val, \"rb\") as pickled:\n",
    "        val_data = pickle.load(pickled)\n",
    "else:\n",
    "    print(\"Creating and saving Validation Dataset\")\n",
    "    val_data = AmazonReviewDataset(path_val)\n",
    "    with open(pickle_path_val, \"wb\") as pickled:\n",
    "         pickle.dump(val_data, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#from os import path\n",
    "#import pickle\n",
    "#\n",
    "#pickle_path_test = \"data.test-0_05_500k.pickle\"\n",
    "#\n",
    "#if path.exists(pickle_path_test):\n",
    "#    print(\"Loading Test Dataset\")\n",
    "#    with open(pickle_path_test, \"rb\") as pickled:\n",
    "#        test_data = pickle.load(pickled)\n",
    "#else:\n",
    "#    print(\"Creating and saving Test Dataset\")\n",
    "#    test_data = AmazonReviewDataset(path_test)\n",
    "#    with open(pickle_path_test, \"wb\") as pickled:\n",
    "#         pickle.dump(test_data, pickled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train_data = AmazonReviewDataset(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#val_data = AmazonReviewDataset(path_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataLoader as well, using a custom collate function for creating the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2tensor(batch):\n",
    "    X, Y = [None]*len(batch), [None]*len(batch)\n",
    "    \n",
    "    for i, row in enumerate(batch):\n",
    "        X[i] = row[0]\n",
    "        Y[i] = row[1]\n",
    "        \n",
    "    return torch.LongTensor(X), torch.LongTensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader_train = DataLoader(train_data, batch_size=1024, shuffle=True, num_workers=4, collate_fn=batch2tensor)\n",
    "dataloader_val = DataLoader(val_data, batch_size=1024, shuffle=True, num_workers=4, collate_fn=batch2tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dataiter_train = iter(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_dash, Y_dash = dataiter_train.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dash = X_dash.cuda()\n",
    "Y_dash = Y_dash.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dash.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, word_embeddings, embedding_size=300, padding=200, category_amount=5, dropout=0.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Predefined word embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.l1 = nn.Linear(embedding_size * padding, 256)\n",
    "        self.l2 = nn.Linear(256, category_amount)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.l1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFN(torch.FloatTensor(embedding_model.vectors)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_dash).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_dash)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_dash).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dash.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0,5), np.exp(model(X_dash).detach().cpu().numpy()[0]), alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.04, nesterov=True, momentum=0.1)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "train_losses, train_accuracies, validation_losses, validation_accuracies = [], [], [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    \n",
    "    for X, Y in dataloader_train:\n",
    "        X = X.cuda()\n",
    "        Y = Y.cuda()\n",
    "    \n",
    "        # Reset Gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward, Loss, Backwards, Update\n",
    "        output = model(X)\n",
    "        loss = criterion(output, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate Metrics\n",
    "        training_loss += loss.item()\n",
    "        training_accuracy += torch.sum(torch.exp(output).topk(1)[1].view(-1) == Y).item()\n",
    "        \n",
    "    else:\n",
    "        validation_loss = 0\n",
    "        validation_accuracy = 0\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X, Y in dataloader_val:\n",
    "                X = X.cuda()\n",
    "                Y = Y.cuda()  \n",
    "                \n",
    "                output_validation = model(X)\n",
    "                loss_val = criterion(output_validation, Y)\n",
    "                validation_loss += loss_val.item()\n",
    "                validation_accuracy += torch.sum(torch.exp(output_validation).topk(1, dim=1)[1].view(-1) == Y).item()\n",
    "        \n",
    "        training_loss /= len(train_data)\n",
    "        training_accuracy /= len(train_data)\n",
    "        validation_loss /= len(val_data)\n",
    "        validation_accuracy /= len(val_data)\n",
    "        \n",
    "        # Saving metrics\n",
    "        train_losses.append(training_loss)\n",
    "        train_accuracies.append(training_accuracy)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        \n",
    "        print(\"Epoch: {}/{}\\n\".format(e+1, epochs),\n",
    "              \"Training Loss: {:.6f}\\n\".format(training_loss),\n",
    "              \"Training Accuracy: {:.3f}\\n\".format(training_accuracy),\n",
    "              \"Validation Loss: {:.6f}\\n\".format(validation_loss),\n",
    "              \"Validation Accuracy: {:.3f}\\n\".format(validation_accuracy))\n",
    "        \n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(0,5), np.exp(model(X_dash).detach().cpu().numpy()[0]), alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dash[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(validation_losses, label='Validation loss')\n",
    "plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(validation_accuracies, label='Validation Accuracy')\n",
    "plt.legend(frameon=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, word_embeddings, embedding_size=300, padding=200,\n",
    "                 category_amount=5, dropout=0.25, lstm_dropout=0.25):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Predefined word embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_embeddings)\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(embedding_size * padding, 256, 2,\n",
    "                           batch_first=True, dropout=lstm_dropout,\n",
    "                           bidirectional=True)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # FFN\n",
    "        self.l1 = nn.Linear(256, category_amount)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(torch.FloatTensor(embedding_model.vectors)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = net.init_hidden(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_dash, hidden).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/flennic/git/text-mining-project/src/data/processed/train.csv\", header=None)\n",
    "test = pd.read_csv(\"/home/flennic/git/text-mining-project/src/data/processed/test.csv\", header=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids1 = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\".lower(), add_special_tokens=True)).unsqueeze(0).to('cuda')  # Batch size 1\n",
    "input_ids2 = torch.tensor(tokenizer.encode(\"Hello, my cat is ugly\".lower(), add_special_tokens=True)).unsqueeze(0).to('cuda')  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat((input_ids1, input_ids2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([0, 1]).unsqueeze(0).to('cuda')   # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, logits = outputs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(logits.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0).to('cuda')  # Batch size 1\n",
    "outputs = model(input_ids)\n",
    "last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Hello, my dog is cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"[CLS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0][0, 0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (\"a\", \"b\", \"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = (1, 2) + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('src/data/embeddings/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[\"hello\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
